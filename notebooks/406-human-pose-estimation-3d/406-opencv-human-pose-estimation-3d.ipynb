{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4031f1e0-69b4-467b-b09d-1eac80fd6609",
   "metadata": {},
   "source": [
    "# Live 3D Human Pose Estimation with OpenVINO\n",
    "\n",
    "This notebook demonstrates live 3D Human Pose Estimation with OpenVINO.We use the model [human-pose-estimation-3d-0001](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/human-pose-estimation-3d-0001) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). At the bottom of this notebook, you will see live inference results from your webcam. You can also upload a video file.\n",
    "\n",
    "> NOTE: _To use the webcam, you must run this Jupyter notebook on a computer with a webcam. If you run on a server, the webcam will not work. However, you can still do inference on a video in the final step.This demo uses a 3D visualization library written by OpenCV to project 3D locations onto a 2D screen. The library location is located in the **notebooks/utils/Visual3D** of the project's home directory._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98ea61-f241-444c-a6e8-f8e9abc61ae6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ad889-8514-430f-baf4-4f32abd43356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import Image, clear_output, display\n",
    "from openvino.runtime import Core, PartialShape\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "\n",
    "sys.path.append(\"../utils/Visual3D\")\n",
    "from cvEngine import Engine3D\n",
    "\n",
    "sys.path.append(\"./modules\")\n",
    "from modules.parse_poses import parse_poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab2b0d-e74f-4872-ad16-3b601e6ba0cf",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "### Download the model\n",
    "\n",
    "We use `omz_downloader`, which is a command line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model.\n",
    "\n",
    "If you want to download another model, please change the model name and precision. *Note: This will require a different pose extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd89c7-be8a-4b03-ba38-c19d328e332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"human-pose-estimation-3d-0001\"\n",
    "# selected precision (FP32, FP16)\n",
    "precision = \"FP32\"\n",
    "\n",
    "model_dir = Path(\"./model\").expanduser()\n",
    "\n",
    "BASE_MODEL_NAME = f\"model/public/{model_name}/{model_name}\"\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pth\")\n",
    "onnx_path = Path(BASE_MODEL_NAME).with_suffix(\".onnx\")\n",
    "\n",
    "ir_model_path = f\"model/public/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/public/{model_name}/{precision}/{model_name}.bin\"\n",
    "video_path = f\"data/face-demographics-walking.mp4\"\n",
    "# video_path = f\"data/kongfu.mp4\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    download_command = (\n",
    "        f\"omz_downloader \"\n",
    "        f\"--name {model_name} \"\n",
    "        #  f\"--precision {precision} \"\n",
    "        f\"--output_dir {model_dir}\"\n",
    "    )\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee24b3-8e53-4ab2-851e-77f5b3dc7170",
   "metadata": {},
   "source": [
    "### Convert Model to OpenVINO IR format\n",
    "We use `omz_converter` to convert the ONNX format model to the OpenVINO format model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bdfdee-c2ef-4710-96c1-8a6a896a8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter \"\n",
    "        f\"--name {model_name} \"\n",
    "        # f\"--precisions {precision} \"\n",
    "        f\"--download_dir {model_dir} \"\n",
    "        f\"--output_dir {model_dir}\"\n",
    "    )\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4364301-3a19-4bcb-bfcf-25c53e72a818",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "Converted models are located in a fixed structure, which indicates vendor, model name and precision.\n",
    "\n",
    "Only a few lines of code are required to run the model. First, we initialize the Inference Engine. Then we read the network architecture and model weights from the .bin and .xml files to compile it for the desired device.Creates an inference request object used to infer the compiled model.The created request has allocated input and output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a04102-aebf-4976-874b-b98dca97ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "ie_core = Core()\n",
    "# read the network and corresponding weights from file\n",
    "model = ie_core.read_model(model=ir_model_path, weights=model_weights_path)\n",
    "# load the model on the CPU (you can use GPU or MYRIAD as well)\n",
    "compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "infer_request = compiled_model.create_infer_request()\n",
    "input_tensor_name = model.inputs[0].get_any_name()\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layers = list(compiled_model.outputs)\n",
    "\n",
    "# get input size\n",
    "height, width = list(input_layer.shape)[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46adaa6-feae-4df0-8dba-4132ed283e02",
   "metadata": {},
   "source": [
    "## Processing\n",
    "### Model Inference\n",
    "The image data is used as the input of the model to obtain the output heatmaps, PAF and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f8055b-a6cf-4003-8232-6f73a86d6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_infer(scaled_img, stride):\n",
    "    \"\"\"\n",
    "    inference\n",
    "\n",
    "    Parameters:\n",
    "        scaled_img: resized image\n",
    "        stride: int, the stride of the window\n",
    "    \"\"\"\n",
    "    global infer_request, input_tensor_name, model\n",
    "\n",
    "    n, c, h, w = model.inputs[0].shape\n",
    "    # print([n, c, h, w ])\n",
    "    img = scaled_img[\n",
    "        0 : scaled_img.shape[0] - (scaled_img.shape[0] % stride),\n",
    "        0 : scaled_img.shape[1] - (scaled_img.shape[1] % stride),\n",
    "    ]\n",
    "\n",
    "    if h != img.shape[0] or w != img.shape[1]:\n",
    "        model.reshape(\n",
    "            {input_tensor_name: PartialShape([n, c, img.shape[0], img.shape[1]])}\n",
    "        )\n",
    "        compiled_model = ie_core.compile_model(model, \"GPU\")\n",
    "        infer_request = compiled_model.create_infer_request()\n",
    "\n",
    "    img = np.transpose(img, (2, 0, 1))[\n",
    "        None,\n",
    "    ]\n",
    "    infer_request.infer({input_tensor_name: img})\n",
    "    results = {\n",
    "        name: infer_request.get_tensor(name).data[:]\n",
    "        for name in {\"features\", \"heatmaps\", \"pafs\"}\n",
    "    }\n",
    "\n",
    "    results = (results[\"features\"][0], results[\"heatmaps\"][0], results[\"pafs\"][0])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def rotate_poses(poses_3d, R, t):\n",
    "    \"\"\"\n",
    "    Rotating human coordinates\n",
    "\n",
    "    Parameters:\n",
    "        poses_3d: The position of the human body in the 3-D coordinate system\n",
    "        R: int, the rotation matrix\n",
    "        t: int, the translation matrix\n",
    "    \"\"\"\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    for pose_id in range(poses_3d.shape[0]):\n",
    "        pose_3d = poses_3d[pose_id].reshape((-1, 4)).transpose()\n",
    "        pose_3d[0:3] = np.dot(R_inv, pose_3d[0:3] - t)\n",
    "        poses_3d[pose_id] = pose_3d.transpose().reshape(-1)\n",
    "\n",
    "    return poses_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3e4b2-34b5-4a91-9608-df17c50b971d",
   "metadata": {},
   "source": [
    "### Draw 2D Pose Overlays\n",
    "We need to define some connections between joints in advance, so that we can draw the structure of the human body in the image when we get the results.\n",
    "Joints are drawn as circles and limbs are drawn as lines. The code is based on the [3D Human Pose Estimation Demo](https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/human_pose_estimation_3d_demo/python) from Open Model Zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd3e08-ed3b-44ac-bd07-4a80130d6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3D edge index array\n",
    "body_edges = np.array(\n",
    "    [\n",
    "        [11, 10],\n",
    "        [10, 9],\n",
    "        [9, 0],\n",
    "        [0, 3],\n",
    "        [3, 4],\n",
    "        [4, 5],\n",
    "        [0, 6],\n",
    "        [6, 7],\n",
    "        [7, 8],\n",
    "        [0, 12],\n",
    "        [12, 13],\n",
    "        [13, 14],\n",
    "        [0, 1],\n",
    "        [1, 15],\n",
    "        [15, 16],\n",
    "        [1, 17],\n",
    "        [17, 18],\n",
    "    ]\n",
    ")\n",
    "\n",
    "body_edges_2d = np.array(\n",
    "    [\n",
    "        [0, 1],  # neck - nose\n",
    "        [1, 16],\n",
    "        [16, 18],  # nose - l_eye - l_ear\n",
    "        [1, 15],\n",
    "        [15, 17],  # nose - r_eye - r_ear\n",
    "        [0, 3],\n",
    "        [3, 4],\n",
    "        [4, 5],  # neck - l_shoulder - l_elbow - l_wrist\n",
    "        [0, 9],\n",
    "        [9, 10],\n",
    "        [10, 11],  # neck - r_shoulder - r_elbow - r_wrist\n",
    "        [0, 6],\n",
    "        [6, 7],\n",
    "        [7, 8],  # neck - l_hip - l_knee - l_ankle\n",
    "        [0, 12],\n",
    "        [12, 13],\n",
    "        [13, 14],\n",
    "    ]\n",
    ")  # neck - r_hip - r_knee - r_ankle\n",
    "\n",
    "\n",
    "def draw_poses(img, poses_2d):\n",
    "    for pose in poses_2d:\n",
    "        pose = np.array(pose[0:-1]).reshape((-1, 3)).transpose()\n",
    "\n",
    "        was_found = pose[2] > 0\n",
    "\n",
    "        # Draw joints.\n",
    "        for edge in body_edges_2d:\n",
    "            if was_found[edge[0]] and was_found[edge[1]]:\n",
    "                cv2.line(\n",
    "                    img,\n",
    "                    tuple(pose[0:2, edge[0]].astype(np.int32)),\n",
    "                    tuple(pose[0:2, edge[1]].astype(np.int32)),\n",
    "                    (255, 255, 0),\n",
    "                    4,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "        # Draw limbs.\n",
    "        for kpt_id in range(pose.shape[1]):\n",
    "            if pose[2, kpt_id] != -1:\n",
    "                cv2.circle(\n",
    "                    img,\n",
    "                    tuple(pose[0:2, kpt_id].astype(np.int32)),\n",
    "                    3,\n",
    "                    (0, 255, 255),\n",
    "                    -1,\n",
    "                    cv2.LINE_AA,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ffb0f-f386-4e7b-8cbf-2ba2fe5607d0",
   "metadata": {},
   "source": [
    "### 3D Visualization library\n",
    "A 3D visualization library is introduced to map 3D coordinates onto a 2D screen.\n",
    "The implementation can be found in the **notebooks/utils/Visual3D** at the root of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a1e46-0776-4ac7-9d71-52f4dda53f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "newEngine = Engine3D(height=300, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb96f7-c65b-4a19-a879-e7f7b35bf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numba as nb\n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "\n",
    "\n",
    "# def normalization(data):\n",
    "#     mu = np.mean(data, axis=0)\n",
    "#     sigma = np.std(data, axis=0)\n",
    "#     return (data - mu) / sigma\n",
    "# def normalization(data):\n",
    "#     _range = np.max(abs(data))\n",
    "#     return data / 400\n",
    "# minmaxData = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc43502-c217-4ecf-9d5d-ce44c104c884",
   "metadata": {},
   "source": [
    "### Main Processing Function\n",
    "\n",
    "Run 3D pose estimation on the specified source. Either a webcam or a video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be526d0-75ad-4bd1-85b1-ca8185eca918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pose_estimation(\n",
    "    source=video_path, flip=False, engine3D=newEngine, use_popup=False\n",
    "):\n",
    "    \"\"\"\n",
    "    draw the results\n",
    "    \"\"\"\n",
    "    base_height = 256  # default\n",
    "    fx = -1  # default\n",
    "    stride = 8\n",
    "\n",
    "    processing_times = collections.deque()\n",
    "    player = None\n",
    "\n",
    "    try:\n",
    "        # create video player to play with target fps  video_path\n",
    "        # get the frame from camera\n",
    "        player = utils.VideoPlayer(source, flip=flip, fps=30, skip_first_frames=0)\n",
    "        # start capturing\n",
    "        player.start()\n",
    "\n",
    "        if use_popup:\n",
    "            # canvas_3d_window_name = 'human pose 3D'\n",
    "            # cv2.cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)(canvas_3d_window_name)\n",
    "            # cv2.setMouseCallback(canvas_3d_window_name, newEngine.camera.control())\n",
    "            canvas_3d_window_name = \"human pose 3D\"\n",
    "            cv2.namedWindow(\n",
    "                canvas_3d_window_name, cv2.WINDOW_FREERATIO\n",
    "            )  # cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.resizeWindow(canvas_3d_window_name, (1000, 1000))\n",
    "\n",
    "        while True:\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            input_scale = base_height / frame.shape[0]\n",
    "            scaled_img = cv2.resize(frame, dsize=None, fx=input_scale, fy=input_scale)\n",
    "            # print(scaled_img.shape)\n",
    "            if fx < 0:  # Focal length is unknown\n",
    "                fx = np.float32(0.8 * frame.shape[1])\n",
    "\n",
    "            # inference start\n",
    "            start_time = time.time()\n",
    "\n",
    "            inference_result = model_infer(scaled_img, stride)\n",
    "            poses_3d, poses_2d = parse_poses(\n",
    "                inference_result, input_scale, stride, fx, True\n",
    "            )\n",
    "\n",
    "            # inference stop\n",
    "            stop_time = time.time()\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # use processing times from last 200 frames\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                (20, 40),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                1,\n",
    "                (0, 0, 255),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "            if len(poses_3d) > 0:\n",
    "\n",
    "                poses_3d[:, 1::4], poses_3d[:, 2::4] = (\n",
    "                    -poses_3d[:, 1::4],\n",
    "                    poses_3d[:, 2::4] - 120,\n",
    "                )\n",
    "                # poses_3d[:, 0::4] = poses_3d[:, 0::4] + 300\n",
    "                # global poses_3d_tmp\n",
    "                # poses_3d_tmp = poses_3d\n",
    "                # poses_3d = poses_3d.reshape(-1, 4)\n",
    "                poses_3d = poses_3d.reshape(poses_3d.shape[0], -1, 4)\n",
    "\n",
    "                poses_3d = normalization(poses_3d)\n",
    "                # global minmaxData\n",
    "                # minmaxData.append((np.min(poses_3d), np.max(poses_3d)))\n",
    "                # print(poses_3d.shape)\n",
    "\n",
    "                newEngine.skeleton.set_body(poses_3d, body_edges)\n",
    "\n",
    "            img = newEngine.image()\n",
    "\n",
    "            # draw 2D\n",
    "            draw_poses(frame, poses_2d)\n",
    "\n",
    "            if use_popup:\n",
    "\n",
    "                cv2.imshow(canvas_3d_window_name, img)\n",
    "                newEngine.camera.control()\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    \".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90]\n",
    "                )\n",
    "\n",
    "                # # escape = 27\n",
    "                # if key == 27:\n",
    "                #     break\n",
    "            else:\n",
    "                frame = cv2.resize(frame, (img.shape[1], img.shape[0]))\n",
    "                stacked_frame = np.hstack((img, frame))\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    \".jpg\", stacked_frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90]\n",
    "                )\n",
    "\n",
    "            display(Image(data=encoded_img))\n",
    "            # display the image in this notebook\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # stop capturing\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9f89f-c65e-4625-abd5-93b442b27e1b",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "### Run Live Pose Estimation\n",
    "\n",
    "Run using a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`.\n",
    "\n",
    "*Note: If you set `use_popup=True`, use your keyboard to move your camera, like:\n",
    "| key | function | key | function |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|'W'|forward|'I'|upward|\n",
    "|'S'|back|'J'|towards the left|\n",
    "|'A'|left|'K'|downward|\n",
    "|'D'|right|'L'|towards the right|\n",
    "|'Q'|up|'E'|down|\n",
    "\n",
    "\n",
    "*Note: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server (e.g. Binder), the webcam will not work.*\n",
    "\n",
    "*Note: Popup mode may not work if you run this notebook on a remote computer (e.g. Binder).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82e298-5912-48c7-90b5-339aea3c177d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_pose_estimation(source=0, flip=True, engine3D=newEngine, use_popup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365628d-f0a1-4b0c-b3bb-0f16a7208874",
   "metadata": {},
   "source": [
    "### Run Pose Estimation on a Video File\n",
    "\n",
    "If you don't have a webcam, you can still run this demo with a video file. Any [format supported by OpenCV](https://docs.opencv.org/4.5.1/dd/d43/tutorial_py_video_display.html) will work. \n",
    "You can click and move your mouse over the picture on the left to interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fddfc-7a23-4897-b57f-81a1ac9aad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pose_estimation(source=video_path, flip=True, engine3D=newEngine, use_popup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env1",
   "language": "python",
   "name": "openvino_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
