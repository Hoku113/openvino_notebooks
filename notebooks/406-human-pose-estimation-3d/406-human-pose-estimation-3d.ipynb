{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de9a93e-9247-4799-a5bb-2ec1575ae8c2",
   "metadata": {},
   "source": [
    "# Live 3D Human Pose Estimation with OpenVINO\n",
    "\n",
    "This notebook demonstrates live 3D Human Pose Estimation with OpenVINO.We use the model [human-pose-estimation-3d-0001](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/human-pose-estimation-3d-0001) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). At the bottom of this notebook, you will see live inference results from your webcam. You can also upload a video file.\n",
    "\n",
    "> NOTE: _To use the webcam, you must run this Jupyter notebook on a computer with a webcam. If you run on a server, the webcam will not work. However, you can still do inference on a video in the final step.  This demo uses WebGL and is intended to work with the browser for better presentation._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9332fb-1cee-4faa-9555-731ddf0e0df7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ad889-8514-430f-baf4-4f32abd43356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from openvino.runtime import Core, PartialShape\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "\n",
    "sys.path.append(\"./modules\")\n",
    "import modules.engine3js as engine\n",
    "from modules.parse_poses import parse_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use visualization library\n",
    "engine3D = engine.Engine3js(grid=True, axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96ad61a-59ff-4873-b2f3-3994d6826f51",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "### Download the model\n",
    "\n",
    "We use `omz_downloader`, which is a command line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model.\n",
    "\n",
    "If you want to download another model, please change the model name and precision. *Note: This will require a different pose extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd89c7-be8a-4b03-ba38-c19d328e332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"human-pose-estimation-3d-0001\"\n",
    "# selected precision (FP32, FP16)\n",
    "precision = \"FP32\"\n",
    "\n",
    "model_dir = Path(\"./model\").expanduser()\n",
    "\n",
    "BASE_MODEL_NAME = f\"model/public/{model_name}/{model_name}\"\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pth\")\n",
    "onnx_path = Path(BASE_MODEL_NAME).with_suffix(\".onnx\")\n",
    "\n",
    "ir_model_path = f\"model/public/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/public/{model_name}/{precision}/{model_name}.bin\"\n",
    "video_path = f\"data/face-demographics-walking.mp4\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    download_command = (\n",
    "        f\"omz_downloader \"\n",
    "        f\"--name {model_name} \"\n",
    "        #  f\"--precision {precision} \"\n",
    "        f\"--output_dir {model_dir}\"\n",
    "    )\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f39f76-2f81-4c18-9fda-98ea6a944220",
   "metadata": {},
   "source": [
    "### Convert Model to OpenVINO IR format\n",
    "We use `omz_converter` to convert the ONNX format model to the OpenVINO format model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bdfdee-c2ef-4710-96c1-8a6a896a8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter \"\n",
    "        f\"--name {model_name} \"\n",
    "        # f\"--precisions {precision} \"\n",
    "        f\"--download_dir {model_dir} \"\n",
    "        f\"--output_dir {model_dir}\"\n",
    "    )\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a07ac-d092-4254-848a-dd48f4934fb5",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "Converted models are located in a fixed structure, which indicates vendor, model name and precision.\n",
    "\n",
    "Only a few lines of code are required to run the model. First, we initialize the Inference Engine. Then we read the network architecture and model weights from the .bin and .xml files to compile it for the desired device.Creates an inference request object used to infer the compiled model.The created request has allocated input and output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a04102-aebf-4976-874b-b98dca97ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "ie_core = Core()\n",
    "# read the network and corresponding weights from file\n",
    "model = ie_core.read_model(model=ir_model_path, weights=model_weights_path)\n",
    "# load the model on the CPU (you can use GPU or MYRIAD as well)\n",
    "compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "infer_request = compiled_model.create_infer_request()\n",
    "input_tensor_name = model.inputs[0].get_any_name()\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layers = list(compiled_model.outputs)\n",
    "\n",
    "# get input size\n",
    "height, width = list(input_layer.shape)[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb5032-a06e-48c1-a3d6-f0fbad9924fb",
   "metadata": {},
   "source": [
    "## Processing\n",
    "### Model Inference\n",
    "The image data is used as the input of the model to obtain the output heatmaps, PAF and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f8055b-a6cf-4003-8232-6f73a86d6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_infer(scaled_img, stride):\n",
    "    \"\"\"\n",
    "    inference\n",
    "\n",
    "    Parameters:\n",
    "        scaled_img: resized image\n",
    "        stride: int, the stride of the window\n",
    "    \"\"\"\n",
    "    global infer_request, input_tensor_name, model\n",
    "\n",
    "    # number, channel, height, width\n",
    "    n, c, h, w = model.inputs[0].shape\n",
    "    # Remove excess space from the picture\n",
    "    img = scaled_img[\n",
    "        0 : scaled_img.shape[0] - (scaled_img.shape[0] % stride),\n",
    "        0 : scaled_img.shape[1] - (scaled_img.shape[1] % stride),\n",
    "    ]\n",
    "    # Adapt the input of the model to the image size\n",
    "    if h != img.shape[0] or w != img.shape[1]:\n",
    "        model.reshape(\n",
    "            {input_tensor_name: PartialShape([n, c, img.shape[0], img.shape[1]])}\n",
    "        )\n",
    "        compiled_model = ie_core.compile_model(model, \"CPU\")\n",
    "        infer_request = compiled_model.create_infer_request()\n",
    "\n",
    "    img = np.transpose(img, (2, 0, 1))[\n",
    "        None,\n",
    "    ]\n",
    "    infer_request.infer({input_tensor_name: img})\n",
    "    # The set of three inference results is obtained\n",
    "    results = {\n",
    "        name: infer_request.get_tensor(name).data[:]\n",
    "        for name in {\"features\", \"heatmaps\", \"pafs\"}\n",
    "    }\n",
    "    # Get the results\n",
    "    results = (results[\"features\"][0], results[\"heatmaps\"][0], results[\"pafs\"][0])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def rotate_poses(poses_3d, R, t):\n",
    "    \"\"\"\n",
    "    Rotating human coordinates\n",
    "\n",
    "    Parameters:\n",
    "        poses_3d: The position of the human body in the 3-D coordinate system\n",
    "        R: int, the rotation matrix\n",
    "        t: int, the translation matrix\n",
    "    \"\"\"\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    for pose_id in range(poses_3d.shape[0]):\n",
    "        pose_3d = poses_3d[pose_id].reshape((-1, 4)).transpose()\n",
    "        pose_3d[0:3] = np.dot(R_inv, pose_3d[0:3] - t)\n",
    "        poses_3d[pose_id] = pose_3d.transpose().reshape(-1)\n",
    "\n",
    "    return poses_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991403a-4f87-45be-9b3f-d30b23a46dbe",
   "metadata": {},
   "source": [
    "### Draw 2D Pose Overlays\n",
    "We need to define some connections between joints in advance, so that we can draw the structure of the human body in the image when we get the results.\n",
    "Joints are drawn as circles and limbs are drawn as lines. The code is based on the [3D Human Pose Estimation Demo](https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/human_pose_estimation_3d_demo/python) from Open Model Zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd3e08-ed3b-44ac-bd07-4a80130d6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3D edge index array\n",
    "body_edges = np.array(\n",
    "    [\n",
    "        [11, 10],\n",
    "        [10, 9],\n",
    "        [9, 0],\n",
    "        [0, 3],\n",
    "        [3, 4],\n",
    "        [4, 5],\n",
    "        [0, 6],\n",
    "        [6, 7],\n",
    "        [7, 8],\n",
    "        [0, 12],\n",
    "        [12, 13],\n",
    "        [13, 14],\n",
    "        [0, 1],\n",
    "        [1, 15],\n",
    "        [15, 16],\n",
    "        [1, 17],\n",
    "        [17, 18],\n",
    "    ]\n",
    ")\n",
    "\n",
    "body_edges_2d = np.array(\n",
    "    [\n",
    "        [0, 1],  # neck - nose\n",
    "        [1, 16],\n",
    "        [16, 18],  # nose - l_eye - l_ear\n",
    "        [1, 15],\n",
    "        [15, 17],  # nose - r_eye - r_ear\n",
    "        [0, 3],\n",
    "        [3, 4],\n",
    "        [4, 5],  # neck - l_shoulder - l_elbow - l_wrist\n",
    "        [0, 9],\n",
    "        [9, 10],\n",
    "        [10, 11],  # neck - r_shoulder - r_elbow - r_wrist\n",
    "        [0, 6],\n",
    "        [6, 7],\n",
    "        [7, 8],  # neck - l_hip - l_knee - l_ankle\n",
    "        [0, 12],\n",
    "        [12, 13],\n",
    "        [13, 14],\n",
    "    ]\n",
    ")  # neck - r_hip - r_knee - r_ankle\n",
    "\n",
    "\n",
    "def draw_poses(img, poses_2d):\n",
    "    for pose in poses_2d:\n",
    "        pose = np.array(pose[0:-1]).reshape((-1, 3)).transpose()\n",
    "\n",
    "        was_found = pose[2] > 0\n",
    "\n",
    "        # Draw joints.\n",
    "        for edge in body_edges_2d:\n",
    "            if was_found[edge[0]] and was_found[edge[1]]:\n",
    "                cv2.line(\n",
    "                    img,\n",
    "                    tuple(pose[0:2, edge[0]].astype(np.int32)),\n",
    "                    tuple(pose[0:2, edge[1]].astype(np.int32)),\n",
    "                    (255, 255, 0),\n",
    "                    4,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "        # Draw limbs.\n",
    "        for kpt_id in range(pose.shape[1]):\n",
    "            if pose[2, kpt_id] != -1:\n",
    "                cv2.circle(\n",
    "                    img,\n",
    "                    tuple(pose[0:2, kpt_id].astype(np.int32)),\n",
    "                    3,\n",
    "                    (0, 255, 255),\n",
    "                    -1,\n",
    "                    cv2.LINE_AA,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6894ce8-ac91-464d-a7f7-54d09f399f4f",
   "metadata": {},
   "source": [
    "### Main Processing Function\n",
    "\n",
    "Run 3D pose estimation on the specified source. Either a webcam or a video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be526d0-75ad-4bd1-85b1-ca8185eca918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pose_estimation(source=0, flip=False, engine3D=engine3D, use_popup=False):\n",
    "\n",
    "    base_height = 256  # default\n",
    "    fx = -1  # default\n",
    "    stride = 8\n",
    "    if use_popup:\n",
    "        # display the 3D human pose in this notebook, and origin frame in popup window\n",
    "        display(engine3D.renderer)\n",
    "        title = \"Press ESC to Exit\"\n",
    "        cv2.namedWindow(\n",
    "            title, cv2.WINDOW_FREERATIO\n",
    "        )  # cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.resizeWindow(title, (1000, 1000))\n",
    "    else:\n",
    "        # set the 2D image box, show both human pose and image in the notebook\n",
    "        imgbox = widgets.Image(format=\"jpg\", height=300, width=400)\n",
    "        display(widgets.HBox([engine3D.renderer, imgbox]))\n",
    "\n",
    "    player = None\n",
    "    line_tmp = None\n",
    "    skeleton = engine.Skeleton(body_edges=body_edges)\n",
    "\n",
    "    try:\n",
    "        # create video player to play with target fps  video_path\n",
    "        # get the frame from camera\n",
    "        # You can skip first N frames to fast forward video. change 'skip_first_frames'\n",
    "        player = utils.VideoPlayer(source, flip=flip, fps=30, skip_first_frames=10)\n",
    "        # start capturing\n",
    "        player.start()\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "\n",
    "        while True:\n",
    "            # grab the frame\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            input_scale = base_height / frame.shape[0]\n",
    "\n",
    "            # resize image and change dims to fit neural network input\n",
    "            # (see https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/human-pose-estimation-3d-0001)\n",
    "            scaled_img = cv2.resize(frame, dsize=None, fx=input_scale, fy=input_scale)\n",
    "\n",
    "            if fx < 0:  # Focal length is unknown\n",
    "                fx = np.float32(0.8 * frame.shape[1])\n",
    "\n",
    "            # inference start\n",
    "            start_time = time.time()\n",
    "            # get results\n",
    "            inference_result = model_infer(scaled_img, stride)\n",
    "            poses_3d, poses_2d = parse_poses(\n",
    "                inference_result, input_scale, stride, fx, True\n",
    "            )\n",
    "            # inference stop\n",
    "            stop_time = time.time()\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # use processing times from last 200 frames\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                # f\"Lib size: {sys.getsizeof(engine3D) / 1024 / 1024:.1f}MB\",\n",
    "                (20, 40),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                1,\n",
    "                (0, 0, 255),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "            if len(poses_3d) > 0:\n",
    "                # From here, you can rotate the 3D point positions using the function \"draw_poses\",\n",
    "                # or you can directly make the correct mapping below to properly display the object image on the screen\n",
    "                poses_3d_copy = poses_3d.copy()\n",
    "                x = poses_3d_copy[:, 0::4]\n",
    "                y = poses_3d_copy[:, 1::4]\n",
    "                z = poses_3d_copy[:, 2::4]\n",
    "                poses_3d[:, 0::4], poses_3d[:, 1::4], poses_3d[:, 2::4] = (\n",
    "                    -z + np.ones(poses_3d[:, 2::4].shape) * 200,\n",
    "                    -y + np.ones(poses_3d[:, 2::4].shape) * 100,\n",
    "                    -x,\n",
    "                )\n",
    "\n",
    "                poses_3d = poses_3d.reshape(poses_3d.shape[0], 19, -1)[:, :, 0:3]\n",
    "                people = skeleton(poses_3d=poses_3d)\n",
    "\n",
    "                try:\n",
    "                    engine3D.scene.remove(line_tmp)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                engine3D.scene.add(people)\n",
    "                line_tmp = people\n",
    "\n",
    "                # draw 2D\n",
    "                draw_poses(frame, poses_2d)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    engine3D.scene.remove(line_tmp)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if use_popup:\n",
    "                cv2.imshow(title, frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27, use ESC to except\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # encode numpy array to jpg\n",
    "                imgbox.value = cv2.imencode(\n",
    "                    \".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90]\n",
    "                )[1].tobytes()\n",
    "\n",
    "            engine3D.renderer.render(engine3D.scene, engine3D.cam)\n",
    "\n",
    "            # # set frames limit, if you need\n",
    "            # frames_processed += 1\n",
    "            # if video_writer.isOpened() and (frames_processed <= 0 or frames_processed <= 2000):\n",
    "            #     video_writer.write(frame)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # stop capturing\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()\n",
    "        if line_tmp:\n",
    "            engine3D.scene.remove(line_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344840a6-9660-4a11-8b05-729ac2969e28",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "### Run Live Pose Estimation\n",
    "\n",
    "Run using a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`.\n",
    "\n",
    "*Note: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server (e.g. Binder), the webcam will not work.*\n",
    "\n",
    "*Note: Popup mode may not work if you run this notebook on a remote computer (e.g. Binder).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82e298-5912-48c7-90b5-339aea3c177d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_pose_estimation(source=0, flip=True, engine3D=engine3D, use_popup=False)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee7b3c4-a143-4d54-bf5d-0aa758f99928",
   "metadata": {},
   "source": [
    "### Run Pose Estimation on a Video File\n",
    "\n",
    "If you don't have a webcam, you can still run this demo with a video file. Any [format supported by OpenCV](https://docs.opencv.org/4.5.1/dd/d43/tutorial_py_video_display.html) will work. \n",
    "You can click and move your mouse over the picture on the left to interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da463932-dee0-4f0f-8379-133d16fce3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pose_estimation(source=video_path, flip=False, engine3D=engine3D, use_popup=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env1",
   "language": "python",
   "name": "openvino_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
